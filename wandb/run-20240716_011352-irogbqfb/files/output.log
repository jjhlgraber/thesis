  | Name                | Type                         | Params | Mode
-----------------------------------------------------------------------------
0 | base_embedder       | SetCNNEmbedder               | 41.3 K | train
1 | contextual_embedder | SimpleAbstractorEncoderLayer | 29.1 K | train
2 | embedder            | Sequential                   | 70.5 K | train
3 | aggregate_seq       | Flatten                      | 0      | train
4 | final_layer         | SetClassifierLayer           | 6.9 K  | train
-----------------------------------------------------------------------------
77.4 K    Trainable params
0         Non-trainable params
77.4 K    Total params
0.310     Total estimated model params size (MB)
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/job/miniconda3/envs/LTN/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  | Name                | Type                         | Params | Mode
-----------------------------------------------------------------------------
0 | base_embedder       | SetCNNEmbedder               | 41.3 K | train
1 | contextual_embedder | SimpleAbstractorEncoderLayer | 29.1 K | train
2 | embedder            | Sequential                   | 70.5 K | train
3 | aggregate_seq       | Flatten                      | 0      | eval
4 | final_layer         | SetClassifierLayer           | 780    | train
-----------------------------------------------------------------------------
71.2 K    Trainable params
0         Non-trainable params
71.2 K    Total params
0.285     Total estimated model params size (MB)
/home/job/miniconda3/envs/LTN/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=2` reached.