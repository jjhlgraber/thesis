  | Name          | Type               | Params | Mode
-------------------------------------------------------------
0 | embedder      | Sequential         | 0      | train
1 | aggregate_seq | Flatten            | 0      | train
2 | final_layer   | SetClassifierLayer | 780    | train
-------------------------------------------------------------
780       Trainable params
0         Non-trainable params
780       Total params
0.003     Total estimated model params size (MB)
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/job/miniconda3/envs/LTN/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  | Name          | Type               | Params | Mode
-------------------------------------------------------------
0 | embedder      | Sequential         | 0      | train
1 | aggregate_seq | Flatten            | 0      | eval
2 | final_layer   | SetClassifierLayer | 780    | train
-------------------------------------------------------------
780       Trainable params
0         Non-trainable params
780       Total params
0.003     Total estimated model params size (MB)
/home/job/miniconda3/envs/LTN/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=2` reached.