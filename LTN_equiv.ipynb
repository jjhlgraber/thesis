{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_fuzzy_ops import ImpliesReichenbachSigmoidal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ltn\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "import globals_ltn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_connectives(connective_dict):\n",
    "    # Define the fuzzy logic operators from the LTN library\n",
    "    Not = connective_dict[\"Not\"]\n",
    "    And = connective_dict[\"And\"]\n",
    "    Or = connective_dict[\"Or\"]\n",
    "    Implies = connective_dict[\"Implies\"]\n",
    "    Equiv = ltn.Connective(ltn.fuzzy_ops.Equiv(And.connective_op, Implies.connective_op))\n",
    "    return Not, And, Or, Implies, Equiv\n",
    "\n",
    "Not, And, Or, Implies, Equiv = define_connectives(globals_ltn.Luk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple MLP\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            self.model.add_module(f'fc{i}', nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            self.model.add_module(f'relu{i}', nn.ReLU())\n",
    "        self.model.add_module('fc_out', nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.model(x))\n",
    "    \n",
    "class flip(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(flip, self).__init__()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return -x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = 2\n",
    "hidden_size = [64, 64, 32, 32, 16, 16]\n",
    "output_size = 1\n",
    "\n",
    "n_objects = 100\n",
    "constants_tensor_single = torch.linspace(-10, 10, n_objects)\n",
    "constants_tensor = torch.cartesian_prod(constants_tensor_single, constants_tensor_single)\n",
    "x1 = ltn.Variable(\"x1\", constants_tensor)\n",
    "\n",
    "\n",
    "pos = 20 * torch.rand(25, input_size) - 10\n",
    "neg = 20 * torch.rand(25, input_size) - 10\n",
    "\n",
    "pos_i = ((pos + 10) // (constants_tensor_single[1] - constants_tensor_single[0])).int()\n",
    "neg_i = ((neg + 10) // (constants_tensor_single[1] - constants_tensor_single[0])).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "lr = 0.0005\n",
    "model1 = SimpleMLP(input_size, hidden_size, output_size)\n",
    "P1 = ltn.Predicate(model1)\n",
    "model2 = SimpleMLP(input_size, hidden_size, output_size)\n",
    "P2 = ltn.Predicate(model2)\n",
    "optimizer = torch.optim.Adam(list(model1.parameters()) + list(model2.parameters()), lr=lr)\n",
    "sat_agg =ltn.fuzzy_ops.SatAgg(ltn.fuzzy_ops.AggregPMeanError(p=1))\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=1), quantifier=\"f\")\n",
    "\n",
    "preds1 = []\n",
    "preds2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds1.append(P1(x1).value.view(n_objects, n_objects))\n",
    "    preds2.append(P2(x1).value.view(n_objects, n_objects))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x1 = ltn.Variable(\"x1\", constants_tensor)\n",
    "        x_pos = ltn.Variable(\"x_pos\", pos)\n",
    "        x_neg = ltn.Variable(\"x_neg\", neg)\n",
    "\n",
    "        fuzzy_theory = []\n",
    "\n",
    "        axiom_symm = Forall([x1], Equiv(P1(x1), P2(x1)))\n",
    "        fuzzy_theory.append(axiom_symm)\n",
    "        \n",
    "        axiom_pos = Forall([x_pos], P1(x_pos))\n",
    "        fuzzy_theory.append(axiom_pos)\n",
    "        # axiom_pos = Forall([x_pos], P(rho_in(x_pos)))\n",
    "        # fuzzy_theory.append(axiom_pos)\n",
    "\n",
    "        axiom_neg = Forall([x_neg], Not(P1(x_neg)))\n",
    "        fuzzy_theory.append(axiom_neg)\n",
    "        # axiom_neg = Forall([x_neg], Not(P(rho_in(x_neg))))\n",
    "        # fuzzy_theory.append(axiom_neg)\n",
    "\n",
    "        axiom_pos = Forall([x_pos], P2(x_pos))\n",
    "        fuzzy_theory.append(axiom_pos)\n",
    "        # axiom_pos = Forall([x_pos], P(rho_in(x_pos)))\n",
    "        # fuzzy_theory.append(axiom_pos)\n",
    "\n",
    "        axiom_neg = Forall([x_neg], Not(P2(x_neg)))\n",
    "        fuzzy_theory.append(axiom_neg)\n",
    "        # axiom_neg = Forall([x_neg], Not(P(rho_in(x_neg))))\n",
    "        # fuzzy_theory.append(axiom_neg)\n",
    "        \n",
    "\n",
    "        # axiom_asym = P(x_asym)\n",
    "        # fuzzy_theory.append(axiom_asym)\n",
    "        # axiom_asym = Not(P(rho_in(x_asym)))\n",
    "        # fuzzy_theory.append(axiom_asym)\n",
    "\n",
    "\n",
    "\n",
    "        satisfiablity = sat_agg(*fuzzy_theory)\n",
    "\n",
    "\n",
    "        loss = 1. - satisfiablity\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            preds1.append(P1(x1).value.view(n_objects, n_objects))\n",
    "            preds2.append(P2(x1).value.view(n_objects, n_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_order import plot_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aab5255570240c7aec57f67c8b61f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', max=1000), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_mp(preds1, pos_examples=pos_i, neg_examples=neg_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0881ef5b8f364453a4d131f44b94dc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', max=1000), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_mp(preds2, pos_examples=pos_i, neg_examples=neg_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[154], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.Tensor([-2, 5]).unsqueeze(0)\n",
    "input_p = torch.Tensor([5, -2]).unsqueeze(0)\n",
    "W1 = nn.Linear(2, 3)\n",
    "relu = nn.ReLU()\n",
    "W2 = nn.Linear(3, 1)\n",
    "ann = nn.Sequential(W1, relu, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = [64,64,64,64,64,64]\n",
    "output_size = 1\n",
    "rho_in = ltn.Function(flip())\n",
    "\n",
    "constants_tensor = torch.linspace(-100, 100, 1000)\n",
    "x1 = ltn.Variable(\"x1\", constants_tensor)\n",
    "\n",
    "# pos = 20 * torch.rand(5) - 10\n",
    "# neg = 20 * torch.rand(5) - 10\n",
    "\n",
    "pos = 100 * torch.rand(3)\n",
    "neg = 100 * torch.rand(3)\n",
    "\n",
    "asym = torch.Tensor([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "lr = 0.0001\n",
    "model = SimpleMLP(input_size, hidden_size, output_size)\n",
    "P = ltn.Predicate(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "sat_agg =ltn.fuzzy_ops.SatAgg(ltn.fuzzy_ops.AggregPMeanError(p=1))\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=1), quantifier=\"f\")\n",
    "\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds.append(P(x1).value)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x1 = ltn.Variable(\"x1\", constants_tensor)\n",
    "        x_pos = ltn.Variable(\"x_pos\", pos)\n",
    "        x_neg = ltn.Variable(\"x_neg\", neg)\n",
    "        x_asym = ltn.Constant(asym)\n",
    "\n",
    "        fuzzy_theory = []\n",
    "\n",
    "        # axiom_symm = Forall([x1], Equiv(P(x1), P(rho_in(x1))))\n",
    "        # fuzzy_theory.append(axiom_symm)\n",
    "        \n",
    "        axiom_pos = Forall([x_pos], P(x_pos))\n",
    "        fuzzy_theory.append(axiom_pos)\n",
    "        # axiom_pos = Forall([x_pos], P(rho_in(x_pos)))\n",
    "        # fuzzy_theory.append(axiom_pos)\n",
    "\n",
    "        axiom_neg = Forall([x_neg], Not(P(x_neg)))\n",
    "        fuzzy_theory.append(axiom_neg)\n",
    "        # axiom_neg = Forall([x_neg], Not(P(rho_in(x_neg))))\n",
    "        # fuzzy_theory.append(axiom_neg)\n",
    "        \n",
    "\n",
    "        # axiom_asym = P(x_asym)\n",
    "        # fuzzy_theory.append(axiom_asym)\n",
    "        # axiom_asym = Not(P(rho_in(x_asym)))\n",
    "        # fuzzy_theory.append(axiom_asym)\n",
    "\n",
    "\n",
    "\n",
    "        satisfiablity = sat_agg(*fuzzy_theory)\n",
    "\n",
    "\n",
    "        loss = 1. - satisfiablity\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "                preds.append(P(x1).value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e59de694bc4a729673f021d49154d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Tensor Index', max=1000), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create sample data\n",
    "tensors = preds\n",
    "x_values = x1.value\n",
    "\n",
    "num_tensors = len(tensors)\n",
    "# Define the update function\n",
    "def update_plot(tensor_index):\n",
    "    plt.plot(x_values, tensors[tensor_index])\n",
    "    plt.scatter(pos, torch.ones_like(pos), c='g')\n",
    "    plt.scatter(neg, torch.zeros_like(neg), c='r')\n",
    "    plt.title(f\"Tensor Index {tensor_index}\")\n",
    "    plt.ylim(-.05, 1.05)\n",
    "    # Show the updated plot\n",
    "    plt.show(block=False)  # Prevent blocking behavior (optional)\n",
    "    plt.show()  \n",
    "\n",
    "slider = IntSlider(min=0, max=num_tensors - 1, step=1, description=\"Tensor Index\")\n",
    "interact(update_plot, tensor_index=slider)  # Empty function to display the plot\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LTN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
