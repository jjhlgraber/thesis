{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import ltn\n",
    "from order_utils import BaselineRelationalModel, BaselineRelationalModelConcat, AbstractorOrderModel, BaselineRelationalIndependentModel, plot_mp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "from order_utils import adjacency_anti_transitive, adjacency_triangular_lattice, adjacency_lattice, \\\n",
    "adjacency_total_order, adjacency_from_string, get_pairs_adjacency, get_samples_adjacency, \\\n",
    "get_constants, get_pairs_total_order, get_samples_total_order, get_eye_3D, get_sat\n",
    "\n",
    "from custom_fuzzy_ops import ImpliesReichenbachSigmoidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/home/job/miniconda3/envs/LTN/lib/python3.9/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "/home/job/miniconda3/envs/LTN/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:512: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4681aa70bf0b4128a4a172cab2f7d340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val_acc            0.3395858108997345\n",
      "        val_loss            1.1024752855300903\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from simple_abstractor import SimpleAbstractorEncoder\n",
    "from set_models import (\n",
    "    SetCNNEmbedder,\n",
    "    SetSequenceModel,\n",
    "    MaxPoolModule,\n",
    "    FirstPoolModule,\n",
    "    MeanPoolModule,\n",
    ")\n",
    "from set_data_lit import SetTriplesDataModule\n",
    "from set_data import SetCardBaseDataset\n",
    "\n",
    "debug = True\n",
    "save = False\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# parameters for both models and data\n",
    "seq_len = 3\n",
    "features_used = [0, 1, 2, 3]\n",
    "feature_states_used = [0, 1, 2]\n",
    "n_feature_states = len(feature_states_used)\n",
    "\n",
    "if n_feature_states > 3:\n",
    "    balanced_subset = True\n",
    "else:\n",
    "    balanced_subset = False\n",
    "\n",
    "use_official_cards = True\n",
    "if use_official_cards:\n",
    "    data_dir = \"data\"\n",
    "    val_split = 0.01\n",
    "    test_split = 0.01\n",
    "else:\n",
    "    data_dir = \"data/custom_cards\"\n",
    "    val_split = 0.005\n",
    "    test_split = 0.005\n",
    "\n",
    "\n",
    "n_features = len(features_used)\n",
    "label_choice = \"features\"\n",
    "\n",
    "# prepare models\n",
    "embedder_kwargs = dict()\n",
    "abstractor_kwargs = {\n",
    "    \"num_layers\": 2,\n",
    "    \"norm\": None,  # Example normalization layer\n",
    "    \"use_pos_embedding\": False,\n",
    "    \"use_learned_symbols\": False,\n",
    "    \"learn_symbol_per_position\": False,\n",
    "    \"use_symbolic_attention\": True,\n",
    "    \"object_dim\": 64,\n",
    "    \"symbol_dim\": 32,  # Using a different symbol dimension\n",
    "    \"num_heads\": 4,\n",
    "    \"ff_dim\": 128,\n",
    "    \"dropout\": 0.1,\n",
    "    \"MHA_kwargs\": {\n",
    "        \"use_bias\": False,\n",
    "        \"activation\": nn.Identity(),  # Different activation function\n",
    "        # \"activation\": nn.Softmax(-1),\n",
    "        # \"activation\": nn.Sigmoid(),\n",
    "        # \"activation\": sparsemax,\n",
    "        \"use_scaling\": True,\n",
    "        \"shared_kv_proj\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "# seq_model_PT = torch.load(\"e2e_long_seq.pth\")\n",
    "\n",
    "cnn = SetCNNEmbedder()\n",
    "# cnn = torch.load(\"./cnn_checkpoints/cnn.pt\")\n",
    "\n",
    "\n",
    "# classifier = seq_model_PT.final_layer\n",
    "seq_model = SetSequenceModel(\n",
    "    base_embedder=cnn,\n",
    "    # base_embedder=None,\n",
    "    contextual_embedder=None,\n",
    "    seq_len=seq_len,\n",
    "    # seq_len_final_layer=1,\n",
    "    seq_len_final_layer=3,\n",
    "    label_choice=label_choice,\n",
    "    n_features=n_features,\n",
    "    n_feature_states=n_feature_states,\n",
    "    # aggregate_seq=MeanPoolModule(),\n",
    "    aggregate_seq=nn.Flatten(),\n",
    "    # classifier=classifier,\n",
    ")\n",
    "\n",
    "# prepare data\n",
    "ds = SetCardBaseDataset(\n",
    "    # image_embedder=cnn,\n",
    "    features_used=features_used,\n",
    "    feature_states_used=feature_states_used,\n",
    "    data_dir=data_dir,\n",
    "    use_official_cards=use_official_cards,\n",
    ")\n",
    "dm = SetTriplesDataModule(\n",
    "    ds,\n",
    "    batch_size=64,\n",
    "    label_choice=label_choice,\n",
    "    balanced_subset=balanced_subset,\n",
    "    balanced_sampling=True,  # Enable balanced sampling\n",
    "    val_split=val_split,\n",
    "    test_split=test_split,\n",
    ")\n",
    "dm.setup()\n",
    "\n",
    "\n",
    "# training\n",
    "if not debug:\n",
    "    logger = WandbLogger(\n",
    "        project=\"first_project\",\n",
    "        name=\"PT_custom_0123\",\n",
    "        # name=\"NONPTcnn_asym_imbal_0123\",\n",
    "    )\n",
    "else:\n",
    "    logger = False\n",
    "\n",
    "trainer_kwargs = dict(\n",
    "    max_epochs=1,\n",
    "    precision=\"16\",\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15),\n",
    "    ],\n",
    "    val_check_interval=25,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(**trainer_kwargs)\n",
    "trainer.validate(seq_model, dm)\n",
    "if not debug:\n",
    "    trainer.fit(seq_model, dm)\n",
    "    trainer.test(seq_model, dm)\n",
    "\n",
    "    if save:\n",
    "        torch.save(seq_model.base_embedder, \"./cnn_checkpoints/cnn_custom_0123.pth\")\n",
    "\n",
    "    logger.experiment.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LTN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
